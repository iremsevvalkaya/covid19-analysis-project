{
    "paper_id": "PMC1513665",
    "metadata": {
        "title": "An XML-based System for Synthesis of Data from Disparate Databases",
        "authors": [
            {
                "first": "Tahsin",
                "middle": [],
                "last": "Kurc",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Daniel",
                "middle": [
                    "A."
                ],
                "last": "Janies",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Andrew",
                "middle": [
                    "D."
                ],
                "last": "Johnson",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Stephen",
                "middle": [],
                "last": "Langella",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Scott",
                "middle": [],
                "last": "Oster",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Shannon",
                "middle": [],
                "last": "Hastings",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Farhat",
                "middle": [],
                "last": "Habib",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Terry",
                "middle": [],
                "last": "Camerlengo",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "David",
                "middle": [],
                "last": "Ervin",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Umit",
                "middle": [
                    "V."
                ],
                "last": "Catalyurek",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Joel",
                "middle": [
                    "H."
                ],
                "last": "Saltz",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Federated and parallel database technologies have been developed by the database management systems community to enable efficient access to distributed data sources.7,8,9,10 Two main types of virtualization are offered by a federated database management system: (1) transparency in the heterogeneity of attribute names, data schemas, and query languages of data sources and (2) masking the distributed nature of the sources, that is, the client sees a unified, virtually centralized system. Our approach draws from the notion of database federation to provide a unified view of disparate data sets. However, the system described in this paper not only allows federation of existing databases, but also enables on-demand creation of distributed databases and integration of user-defined data processing with data storage.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 166,
                    "mention": "7",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 167,
                    "end": 168,
                    "mention": "8",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 169,
                    "end": 170,
                    "mention": "9",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 171,
                    "end": 173,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Model-management tools11,12 and mediator-based systems have also been employed for integration of semantic information and data across heterogeneous data sets.13 The mediator-based architecture developed by Lud\u00e4scher et al.14 provides support for creation, management, and querying of integrated view definitions. Their system enables linking of data at the semantic level that encodes the domain specific knowledge about data elements and their relationships. Mediation-based knowledge integration layers need to interact with a data management layer to access data sources; that is, semantic queries submitted by clients through a portal interface are translated into queries against databases controlled by the data management layer. Our system addresses the requirements of the data management layer.",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 24,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 25,
                    "end": 27,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 159,
                    "end": 161,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 223,
                    "end": 225,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Grid computing has emerged as a widely accepted mechanism to harness computing, storage, and data resources that are hosted at different locations and connected over wide area networks (e.g., the Internet).15,16 As Grid computing has become more prevalent, a service-oriented view of the Grid has been proposed. The Open Grid Services Architecture (OGSA),17,18 the core set of standards developed by the Global Grid Forum (GGF),19 builds on and extends the Web Services technologies20 to address standard mechanisms and definitions for creating, naming, and integrating Grid services. There are some recent efforts to develop database technologies on Grid and Web services. Bell et al.21 develop interfaces, data models, and security support for relational databases using Web services. Smith et al.22 address the problems associated with distributed execution of queries in a Grid environment. They describe an object-oriented database prototype running on Globus.23 Narayanan et al.24 implement a Grid-enabled infrastructure to support management and querying of scientific data sets stored in distributed collections of flat files. The OGSA Data Access and Integration Services25 (DAIS) working group of the GGF is a focused effort that has been developing the service definitions and standards, drawing from the core OGSA standards, for data access and integration in the Grid. Our software system has been designed as a Grid-aware system and can take advantage of emerging standards. In fact, the XML-based data management system employed in this paper builds on the evolving OGSA-DAIS standards.",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 208,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 209,
                    "end": 211,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 355,
                    "end": 357,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 358,
                    "end": 360,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 428,
                    "end": 430,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 482,
                    "end": 484,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 685,
                    "end": 687,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 799,
                    "end": 801,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 965,
                    "end": 967,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 984,
                    "end": 986,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1180,
                    "end": 1182,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "A number of large projects have been driven by the need to access distributed repositories. The Biomedical Informatics Research Network (BIRN)26 project, funded by the National Institutes of Health (NIH), targets shared access to medical data in a wide-area environment. The BIRN focuses on support for data sets generated by neuroimaging studies. The Shared Pathology Informatics Network (SPIN)27 initiative is intended to provide a virtual database of human tissue specimens. It develops an Internet-based software infrastructure to support a network of tissue specimen data sets and the clinical information associated with these data sets. It allows searches and requests from approved researchers into these data sets while making sure that patient confidentiality requirements are met. Several multi-institutional research projects have been funded by the European Union to investigate the application of Grid technologies for manipulating large medical image databases.28,29,30 The caBIG is an initiative supported by the National Cancer Institute (NCI). The goal of this initiative is to develop applications and the underlying systems architecture for a nationwide cancer research network. The caBIG Grid software infrastructure (called caGrid) will facilitate distributed management and sharing of a vast array of data sources and analytical tools hosted at multiple institutions. The Mobius Global Model Exchange (GME) component of our framework is used in the caGrid infrastructure to manage XML schemas, which represent the structure of data objects that are managed and shared in the caBIG environment.",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 144,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 395,
                    "end": 397,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 976,
                    "end": 978,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 979,
                    "end": 981,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 982,
                    "end": 984,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "One of the challenges that an application developer has to overcome is the fact that different storage formats and database management technologies may be employed by each data source. Querying and accessing data in such a setting become challenging, as applications need to interact with different types of systems. When a new data source is incorporated into the environment, application developers have to update their applications to interact with the new data source correctly. The goal of virtualization is to hide the heterogeneity of software systems used by different data sources by enabling access to the sources with well-defined interfaces and common information exchange protocols. In our framework, this is achieved by services and communication protocols that expose data sources as XML data sources.",
            "cite_spans": [],
            "section": "Virtualization ::: Design Objectives",
            "ref_spans": []
        },
        {
            "text": "In a setting where data can be consumed by disparate clients, it is important to be able to define the structure of a data type and to publish and manage this definition. A schema provides a formal and complete representation of the structure of a data type and can act as a contract of data representation between data producers and data consumers. In this way, a client can correctly interpret a data object served by a data source and client programs can interact with the data source programmatically. Our framework provides support for creating, publishing, and managing data types as XML schemas in a distributed environment.",
            "cite_spans": [],
            "section": "Management of Data Types ::: Design Objectives",
            "ref_spans": []
        },
        {
            "text": "Virtualization provides a mechanism to hide the complexity and heterogeneity of external data sources. However, there are cases in which it is desirable to create local caches of data from multiple data sources.Snapshots: In some applications it is important to be able to maintain a local snapshot of the data source. For instance, a community of researchers may want to compare the performance of various statistical or data mining algorithms directed at a static data set. Such studies often involve repeated requests to the database. By storing a snapshot of a data source, the query load on the original data source can be reduced.Interface stability: Both the data attributes and the interface of a data source may change over time. In such cases, it is useful to maintain a data source view on which analysis operations can continue to function.Query optimization: A data source may provide suboptimal infrastructure for associative queries. For example, the data source might allow a client to only download or upload files, despite the fact that these files can be structured documents and queried. By creating an optimized database on the contents of such files, complex queries can be executed efficiently.",
            "cite_spans": [],
            "section": "On-demand Creation of Local Caches of External Data ::: Design Objectives",
            "ref_spans": []
        },
        {
            "text": "Processing of data by user-defined operations is a common aspect of data analysis in almost every application domain. In the context of data management and integration, user-defined functions can be implemented to extract the data of interest from one or more data sources and transform them into a format that complies with published schemas and is more efficient to manage and query. It is also desirable (and necessary in some cases) to be able to compose multiple user-defined operations into a data flow network and take advantage of distributed and parallel computing clusters for more efficient processing.",
            "cite_spans": [],
            "section": "Distributed Execution of User-defined Operations ::: Design Objectives",
            "ref_spans": []
        },
        {
            "text": "Mobius adopts the philosophy that data standards should be allowed to evolve organically, but they should be managed under a common infrastructure. In this vein, Mobius implements a distributed service, the GME, for schema management. The GME provides a well-defined protocol and service implementation for publishing, versioning, and discovering XML schemas in a distributed environment. A schema can be a completely stand-alone description of a particular data set or it can be an elaborate composition of new attributes and references to multiple existing schemas. By referencing other schemas and entities in other schemas, a user can compose more complex data types from simple data types. Since references from a schema to other schemas are allowed, it is essential to ensure referential integrity. Once a schema is published to a GME instance, it is assigned a version and made available for use by other clients. Once published, a schema cannot be modified; however, a new version of the schema can be published.",
            "cite_spans": [],
            "section": "Mobius Global Model Exchange ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "The architecture of GME is similar to the Domain Name Server (DNS). A schema published through a GME instance has to be registered under a namespace. Namespaces enable publishing and management of schemas in a controlled way. A hierarchy of namespaces can be created (like domain names in DNS) and managed by multiple GMEs, each of which is an authority for a set of namespaces and delegates responsibility of subnamespaces to subordinate GMEs. Any published schema can be discovered or resolved through any given GME instance, as the authority hierarchy can be navigated, while the storage of schemas can be distributed across a collection of GMEs.",
            "cite_spans": [],
            "section": "Mobius Global Model Exchange ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "It is reasonable to expect that data types captured in a study will evolve over time; new data attributes may be added or existing attributes may be modified or deleted. It is therefore necessary to support evolution of schemas in a controlled way so that existing programs do not break when a schema goes through changes. The GME architecture formalizes the concept of versions. Any changes made to a data schema reflect either a new version of that schema or a completely new schema under a different name or namespace. This restriction is an important one because it facilitates the evolution of data types while enabling clients and services to still make use of the older versions of the schema.",
            "cite_spans": [],
            "section": "Mobius Global Model Exchange ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "A critical component in supporting access to heterogeneous data sources is the mechanism by which the data sources are exposed (or virtualized). Mako is a strongly typed data storage service that provides a set of well-defined interfaces that expose a data source as an XML data source; since Mako is a distributed service, its interfaces are motivated by the work of the Data Access and Integration Services working group in the Global Grid Forum.33 The Mako client interfaces are similar to those of an XML database. For example, a relational or object database, once exposed through Mako service interfaces, could be queried using XPath as opposed to SQL or OQL (Object Query Language). When an XPath query is received by Mako, it is translated into the query language of the back-end database system; our current implementation of Mako provides support for accessing XML views of existing relational databases via the XQuark Bridge tool (www.xquark.org) and native XML databases via the XMLDB API standard.",
            "cite_spans": [
                {
                    "start": 448,
                    "end": 450,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Mobius Mako: Data Storage and Retrieval ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "Virtualization of data sources as XML data sources trades off some amount of native optimization and expressivity of the underlying data resource in favor of uniformity of data access. This trade-off is acceptable in most cases, since in the process of virtualization, the salient features of the individual data sets of interest have been made prominently available and accessible in the data model being exposed to the client. Often underlying data are aggregated, projected, and denormalized into the corresponding XML view, making attributes of interest easily accessible via XPath queries. While this approach has been very successful, we have identified that there are still occasions when a higher level of expressivity is required by some clients and data aggregation scenarios. The root of this limitation is that XPath does not support expression of joins between documents and document collections. It is solely a data access or location language. Queries, therefore, are limited to accessing the data model as designed by the virtualization process. To support more complex user-defined filters, joins, and more complex queries, we are in the process of developing XQuery support in Mobius.",
            "cite_spans": [],
            "section": "Mobius Mako: Data Storage and Retrieval ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "Virtualization, using Mako, of a data source to a common access model makes it easy and uniform for applications, services, and clients to interact with the data source. Nevertheless, for the service provider, the problem of accessing the native data source remains. Mako abates this problem by providing implementations for common data storage mechanisms (e.g., support for relational databases using the XQuark Bridge tool) and a simple method for extension to new storage and data management systems. The implementation of Mako also provides a custom back end named MakoDB, which is an XML database layered on top of MySQL and optimized for data interactions within the Mako framework. MakoDB provides many advanced features such as (1) the ability to uniquely reference and retrieve individual elements, (2) support for on-demand creation of optimized databases from XML schemas (published in GME), and (3) efficient storage and retrieval of binary data.",
            "cite_spans": [],
            "section": "Mobius Mako: Data Storage and Retrieval ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "The structure of a database in Mako is required to comply with a schema registered in the GME. In this way, Mako enforces all data sets stored and exposed to the Grid to conform to strongly typed, published, and discoverable data models. Each data collection in Mako can be restricted to only accept XML documents from a set of certain schemas. When an XML document conforming to a schema is submitted to a Mako server that accepts said schema, the document is stored and indexed so that the instance data in the document can be queried and subsets of the data can be retrieved.",
            "cite_spans": [],
            "section": "Mobius Mako: Data Storage and Retrieval ::: Mobius Framework ::: System Description",
            "ref_spans": []
        },
        {
            "text": "Our implementation consists of three main services: (1) a metadata service, (2) a data service, and (3) a distributed execution service. The metadata service is built on the GME and provides tools to support management of data models in a distributed environment. An application developer can create, register, and modify schemas that define the structure of data sets managed by the data service. The data service implements tools for efficient storage, management, and querying of distributed data sets. The data service is layered on the Mobius Mako service and designed to take advantage of aggregate storage capacity of distributed disk-based storage platforms and clusters. Using the on-demand database creation capabilities of Mako, local caches of data stored in external data repositories can be created and managed in this service. The distributed execution service is designed to allow execution of user-defined data processing procedures on compute clusters. A more detailed description of the distributed execution service and the underlying runtime system can be found in our earlier work.34,35 The distributed execution service works in tandem with the metadata and data services. The input and output of a data processing component can be described using XML schemas to enable better type checking. Moreover, data output from an application component can be stored in databases maintained in the data service.",
            "cite_spans": [
                {
                    "start": 1103,
                    "end": 1105,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1106,
                    "end": 1108,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "System Implementation ::: System Description",
            "ref_spans": []
        },
        {
            "text": "An application developer using our system is expected to implement application-specific schemas for data types and user-defined operations for data filtering and format transformations. As an example, consider integration of information from protein databases from SwisProt, gene data from the National Center for Biotechnology Information (NCBI) Gene, genome annotations from the University of Southern California (USC), and pathway information from the Kyoto Encyclopedia of Genes and Geonomes (KEGG). In this case, the developer will need to create XML schemas that define each data type (i.e., protein, gene, genome annotations, and pathways) and define join attributes that will be common across these data types. These schemas can be registered in the metadata service. The next step would be to develop customized Mako services that will expose each data source (SwisProt, NCBI Gene, USC, KEGG data servers) as XML data sources that conform to the schemas registered in the metadata service. Alternatively, the application developer can implement data extraction and translation components, which will interact with the individual data sources and create XML documents that conform to the schemas registered in the metadata service. Using the distributed execution service, the data extraction and translation components can be executed on a cluster system and the local caches of the remote sources can be created. User-defined operations are not limited to format translations only; more complex operations (e.g., translation of mass spectral data to predicted peptide sequence) can also be implemented, registered, and executed in the system.",
            "cite_spans": [],
            "section": "System Implementation ::: System Description",
            "ref_spans": []
        },
        {
            "text": "Clearly, there are several steps and application-specific implementations that an application developer has to do before being able to implement support for data integration and management in the application. However, once the data sources have been exposed using common protocols and service interfaces and/or the local caches of the remote data sets have been created, many client programs can be developed without needing to customize each program for each data source. In addition, since the data types are published in the metadata service and data sources are accessed via unified protocols and service interfaces, other researchers also can develop applications that can access the same data. These are the main strengths of our framework. Using our framework, application developers, data providers, and users in a distributed environment can define, publish, and share common data types and implement uniform mechanisms for data source access. This enables data integration, and client programs can operate, even if the back-end data server technologies are different and changed over time.",
            "cite_spans": [],
            "section": "System Implementation ::: System Description",
            "ref_spans": []
        },
        {
            "text": "We have implemented a method of phenotype\u2013genotype correlation based on phylogenetic optimization of large data sets of mouse SNPs and phenotypic data. We have adopted a phylogenetic approach because the hereditary relationships of laboratory mice strains mean that they are not statistically independent entities, thus rendering standard statistical tests inappropriate.42 Our method takes a global approach by optimizing the SNP data into phylogenetic trees to minimize the mutations necessary to explain the observed variation. Phylogenetics sorts variation into attributes that result from common ancestry and those that have evolved independently. In doing so, it can identify mutations that occur in step with the phenotypic changes of interest. Once a set of putative phenotype\u2013genotype correlations has been established, we assess the statistical strength of the correlation and the human biomedical relevance of candidate genes. This filtering process needs to take into account both phylogenetic tree\u2013related statistics, merged with preexisting information contained in mouse and human gene annotations. The results can be used to (1) identify candidate genes that are already known to be relevant to the human disease under study, (2) detect novel candidate genes that are then prioritized for empirical follow-up, and (3) identify false positives. While some results are well-known candidate genes for CAD, other genes have not previously been described as candidates. Such novel candidates are prioritized for empirical follow-up.",
            "cite_spans": [
                {
                    "start": 371,
                    "end": 373,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                }
            ],
            "section": "Application Overview ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "Our application provides the researcher with a mechanism to systematically support the integration of locally generated experimental information, with information obtained from diverse sources. The application consists of two main parts. In the first part, a researcher calculates one or more phylogenetic trees from SNPs using a phylogenetic tree optimization program called POY.43 The researcher may incorporate phenotypic data into the construction of the trees or optimize the phenotypic data onto a tree based only on genotypic data. Once a satisfactory tree has been found, the researcher creates, using POY and the phylogenetic tree, a table of inferred mutations and phenotypic changes among strains of mice (an example of the table is provided in \u25b6). The inferred mutations and the phenotypic changes are then examined using Maddison's44 concentrated changes test to score putative correlations among the mutations and the phenotypic changes. Any highly correlated SNPs are output for each phenotype. The actual implementation of these steps is explained for our use cases in the section \u201cFinding Single Nucleotide Polymorphisms of Interest in Specific Use Cases.\u201d The second part of the application carries out an evaluation of the biomedical relevance of the candidate SNPs. Using a graphical user interface, the researcher selects the list of well-correlated SNPs to guide the search for haplotype blocks. The researcher then gathers annotation data on the biomedical relevance of genes contained in the haplotype blocks from multiple sources, including Mouse Genome Informatics (MGI).45 Finally, the researcher assesses the relevance of the candidate gene(s) based on the annotation data, primary biomedical literature, and the presence of human orthologs. These steps are described in detail in the section \u201cEstimating Haplotypes in Strains of Interest and Using Annotation Data to Assess Biomedical Relevance of Candidate Genes.\u201d Execution of all the steps in the application requires sharing data among the researchers responsible for various analyses and the integration of the results with public data sources.  ",
            "cite_spans": [
                {
                    "start": 380,
                    "end": 382,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 844,
                    "end": 846,
                    "mention": "44",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 1597,
                    "end": 1599,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                }
            ],
            "section": "Application Overview ::: Status Report",
            "ref_spans": [
                {
                    "start": 756,
                    "end": 757,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The first part of our application searches for mutations that change in step with phenotypes among diverse strains of mice. In this paper, we used data sets from the Mouse Phenome Database (MPD) and GNF2. To facilitate the study of complex genetic diseases in mouse models, the Jackson Laboratory has compiled an extensive MPD.4 Phenotype data from many mouse strains are collected from the literature and via collaboration with experts through consistently applied protocols. The data are compiled into a database and flat files for download. In addition, a large number of SNP data sets are now publicly available in the MPD. The mpd146 data set available in the MPD is a compilation of 439,942 single and multiple nucleotide polymorphisms genotyped by many research groups for 17 mouse strains. We analyzed SNP data for 15 strains represented in mpd146, for which there were accompanying phenotype data in the MPD. The phenotype data were selected from a lipid study data set titled \u201cPaigen2\u201d46 in the MPD. The GNF2 database contains 8,944 SNPs.38 Although the GNF2 data have fewer SNPs, the data sets are more uniform in strain coverage, include a larger number of strains (a total of 48 strains), and are more evenly distributed along the genome. Phenotype data were available in the MPD for 39 of the strains represented in the GNF2.",
            "cite_spans": [
                {
                    "start": 327,
                    "end": 328,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 995,
                    "end": 997,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1048,
                    "end": 1050,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Finding Single Nucleotide Polymorphisms of Interest in Specific Use Cases ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "In the application, we use the commands -replicate -tbr -spr of POY43 to simultaneously align the sequence data for characters longer than 1 as various trees are constructed and refined by POY in searching for an optimal tree and alignment. The need for an algorithm such as POY to coordinate alignment and tree construction arises from length differences in multiple nucleotide polymorphisms (MNP) that are part of mpd146. In the case of the GNF2 data, the alignment is fixed because each polymorphism is of length 1 (i.e., an SNP). As a result of the organized nature of the GNF2 data set, data prealigned by any tree search program (such as TNT47) could be used as input. To efficiently store and query the POY output, we developed a data model (\u25b6) and the corresponding XML schema. This model represents a view of the POY output that accommodates a forest, containing one or more trees, implied by the data and analytical parameters (e.g., the number of replicates examined and edit costs for transformations imposed). The data model also includes information on the branches of each tree that represent inferred ancestor\u2013descendant relationships. For each branch, transformations in the nucleotide and phenotype data from the inferred ancestral states to descendant states are assigned and output by POY. The transformations considered in the use cases include mutations, insertion events, or deletion events in nucleotide data and changes in phenotypes (e.g., from normal to elevated non\u2013high-density lipoprotein (HDL) cholesterol plasma levels). A branch may contain multiple transformations (\u25b6).",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 69,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 647,
                    "end": 649,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                }
            ],
            "section": "Finding Single Nucleotide Polymorphisms of Interest in Specific Use Cases ::: Status Report",
            "ref_spans": [
                {
                    "start": 749,
                    "end": 750,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1600,
                    "end": 1601,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Our phylogenetic tree schema complements the schema being developed in the TreeBase component of the CIPRes project (http://www.phylo.org/). Our schema supports transformations assigned to various branches that occur in trees and their implied alignments. The TreeBaseII schema, on the other hand, expresses data types and attributes for studies and related publication references, analyses, matrices, taxon labels, and trees. To have a more comprehensive data model for phylogenetic trees, our data model could be combined with the TreeBaseII schema to capture transformations, implied alignments, and information on studies and publications.",
            "cite_spans": [],
            "section": "Finding Single Nucleotide Polymorphisms of Interest in Specific Use Cases ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "Once phylogenetic trees are constructed, we need to assess the potential that a phenotypic trait is correlated with a DNA polymorphism by chance. To assess the significance of a given SNP, we use Maddison's44 concentrated changes test (CCT). The CCT is a measure of correlation between two binary characters on a phylogenetic tree. It produces a p-value for the type one error on the null hypothesis that the DNA polymorphism is associated with the trait by chance. Since the CCT works on binary data values, the phenotype data are made suitably binary by using a threshold value such as exceeding a standard deviation above or below the mean (e.g., the mean value and standard deviation value of non-HDL cholesterol levels) (\u25b6). The SNPs are often naturally biallelic and thus binary. The CCT determines whether the observed changes in a binary character are concentrated on the branches of a tree that has a particular state of the second character. A step in computing the CCT is to extract the sets of transformations from the branches of interest from the phylogenetic tree and compute intersections and differences of these sets. For instance, one can look for transformations that occur in a group of branches but do not occur in another branch. In our implementation, the phylogenetic tree output from POY can be queried using XPath to search for and extract the branches of interest. The CCT calculations in our case determine whether changes in phenotype are concentrated on the branches that have a derived allele for an SNP of interest (i.e., a recent mutation). We count the number of gains and losses of the phenotypic character over the whole tree and the number of these gains and losses that fall on branches reconstructed to have the derived allele for an SNP of interest.  ",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 208,
                    "mention": "44",
                    "ref_id": "BIBREF38"
                }
            ],
            "section": "Finding Single Nucleotide Polymorphisms of Interest in Specific Use Cases ::: Status Report",
            "ref_spans": [
                {
                    "start": 726,
                    "end": 727,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Although the CCT is implemented in Maddison et al.,48 a user can only calculate the CCT for one pair of characters at a time. In addition, counting the gains and losses of the dependent character has to be done manually. In the research we describe here, we calculate CCTs for very large numbers of SNPs and phenotypes using the macro language in a command line\u2013driven version of Tree analysis using New Technology (TNT).47 Given that there were often several most parsimonious optimizations of an SNP, we used the delayed transformation (DELTRAN)48 optimization to obtain a unique reconstruction of each SNP. Once compiled, the CCT values are stored in the data management and integration system. We have developed a simple schema that consists of the following attributes: cct_value, character, position, trait, and SNP id. The SNPs can be filtered using XPath queries into the CCT data stored in the system (e.g., /snp[(traits/trait/@cct_value < \u201c.05\u201d)] will return SNPs that have significant p-values for the concentrated changes test).",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 53,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 421,
                    "end": 423,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 547,
                    "end": 549,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                }
            ],
            "section": "Finding Single Nucleotide Polymorphisms of Interest in Specific Use Cases ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "Portions of the genomes of inbred mouse strains are often similar over large distances, complicating attempts to finely map the genetic basis of traits.49 Thus, in silico approaches that identify correlated markers must also consider the genomic context of those markers.37,38 Although SNPs vary in density throughout the mouse genome, they lie within genes or in intergenic regions, thus providing the potential for resolution at the locus level (defined by the SNP and flanking DNA). However, the genomic context of the SNP of interest is very important because an SNP may vary with a phenotype, yet may only be associated with the causal gene via linkage disequilibrium. We have implemented a simple algorithm that takes an SNP of interest from the previously described steps as input (e.g., rs3023213 from \u25b6) and returns a range on the genome containing 1 or more markers, thus defining a region of similarity between the query strain pair (e.g., C57BL/6J and CAST/EiJ from \u25b6). Because this process is repeated many times for different regions resulting in many potential candidate genes, an integrated database support was designed.",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 154,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 271,
                    "end": 273,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 274,
                    "end": 276,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Estimating Haplotypes in Strains of Interest and Using Annotation Data to Assess Biomedical Relevance of Candidate Genes ::: Status Report",
            "ref_spans": [
                {
                    "start": 810,
                    "end": 811,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 978,
                    "end": 979,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "To evaluate the potential biomedical relevance of candidate genes that fall within the region of similarity for the phenotype of interest, a researcher needs to retrieve and organize knowledge from various data sources. A challenging issue is the need for querying and integrating data from several public data sets. Internet-based biomedical and genomic data repositories differ, in both data formats and mechanisms by which clients can query and retrieve data. A data source might be accessed via a Web service interface or, alternately, the data source might be a Web or ftp site from which the data set of interest needs to be downloaded. Our approach to supporting integration of data from multiple resources and complex queries is to create caches of subsets of external data sources in our data management and integration system.",
            "cite_spans": [],
            "section": "Estimating Haplotypes in Strains of Interest and Using Annotation Data to Assess Biomedical Relevance of Candidate Genes ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "Data collection from external data sources can be done manually or can be automated in our system. In the manual mode, the user can download a data set from an external data source and load it into the system using one of the available data extraction and loading programs. The data loading programs identify the input data set format and generate a set of XML documents, which are then stored in Mako servers. In the automated mode, our system can support processes, referred to as spiders (or data extractors), that scrape the individual data resources for data. A data source catalog maintains the data access method or the Web site wrapping service to retrieve data files from an Internet repository and the extractor method to parse data attributes and attribute values from the retrieved data files. The distributed execution service can be used to execute spiders and data extraction and loading programs on a PC cluster. In this way, multiple data sources can be accessed and processed concurrently. A spider for each data resource can be executed to obtain the relevant data. The results, returned by each spider, are mapped onto the corresponding data model using the data extraction and loading methods, and then are stored in the system.",
            "cite_spans": [],
            "section": "Estimating Haplotypes in Strains of Interest and Using Annotation Data to Assess Biomedical Relevance of Candidate Genes ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "We have implemented data models for data subsets from different sources so that the contents of data files downloaded from a data source can be stored in the system and queried along with other information. These data models are described as XML schemas, which are stored and managed by the metadata service. Once a set of files from an external data source are downloaded, they are parsed using the corresponding extractor method and XML documents, which conform to the corresponding data model (XML schema) and are created and stored on Mako servers. In our current implementation, we have parsers and data models for a number of databases that can be downloaded from the Jackson Laboratory and for the GNF2 data set. These data sets include MPD-merged mouse strain SNPs (mpd146), Gene Ontology terms data set, Human and Mouse Orthology with Human Online Mendelian Inheritance in Man (OMIM) IDs, MGI Sequence Coordinates, and Human and Mouse Orthology with Sequence information (\u25b6). The data models of the mpd146 and HMD Human Sequence data sets are shown in \u25b6 as examples. Clients can access and query the databases through a graphical user interface that allows execution of a number of query templates (\u25b6).    ",
            "cite_spans": [],
            "section": "Estimating Haplotypes in Strains of Interest and Using Annotation Data to Assess Biomedical Relevance of Candidate Genes ::: Status Report",
            "ref_spans": [
                {
                    "start": 1061,
                    "end": 1062,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1208,
                    "end": 1209,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 981,
                    "end": 982,
                    "mention": "\u25b6",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Detailed results obtained through this application will be presented elsewhere (Habib et al., in preparation). Here we present an example of the types of results obtained. An example query (rs3023213) identified NNMT (nicotinamide N-methyltransferase) as a candidate gene for high non-HDL levels in female mice of strains C57BL/6J and CAST/EiJ, within a block on mouse chromosome 9. NNMT is highly expressed in liver tissue and is known to exhibit large differences, in level and activity, between mouse strains and genders51 and among humans.52 N-methyltransferases (e.g., NNMT) are involved in the biochemical synthesis of homocysteine, a cardiovascular disease risk factor. NNMT was recently implicated as a genetic factor for plasma homocysteine levels, in a genome-wide linkage study in humans.53 The potential link between N-methyltransferases, homocysteine, and cholesterol levels is supported by findings in a knockout mouse model.54",
            "cite_spans": [
                {
                    "start": 523,
                    "end": 525,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 543,
                    "end": 545,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 799,
                    "end": 801,
                    "mention": "53",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 939,
                    "end": 941,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                }
            ],
            "section": "Estimating Haplotypes in Strains of Interest and Using Annotation Data to Assess Biomedical Relevance of Candidate Genes ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "In the first set of experiments, the performance of the system in the format translation and on-demand database creation (data loading) step is examined. \u25b6 shows the execution time for data sets extracted from the mpd146, MGI Gene Association, and MGI coordinate databases from the Jackson Laboratory. In these experiments, the data extractor was run on a machine with a Pentium IV 3-GHz central processing unit (CPU) and 1,024 MB of memory. The data server machine was an SMP node with dual Xeon 2.4-GHz CPUs and 2,048 MB of memory. The two machines were connected to each other over a Gigabit switch. All timings are in seconds and represent the execution time from a single run for each data set. In our implementation, the extractor reads one or more data rows from the data set file and transforms them into XML documents, conforming to the corresponding schemas. When an XML document is created, the extractor submits the document to a Mako server over the network for storage. It can be expected that running the data extractors and the data servers on two separate machines will incur communication overhead. To reduce this overhead, these two components could be colocated. We should note that our implementation allows for placement of a data extractor on the machine where the data server is. However, in many cases, machines hosting data servers may not be configured to run user-defined operations because of security and performance concerns. Our experiments emulate such a configuration.  ",
            "cite_spans": [],
            "section": "Performance: Creation of Local Caches and Data Querying ::: Status Report",
            "ref_spans": [
                {
                    "start": 154,
                    "end": 155,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "As is seen in \u25b6, the bulk of the execution time is spent submitting XML documents to the data server; the time to parse a data file and create XML documents takes less than 5% of the overall execution time on average. In this experiment, the data extraction and loading time for the Gene Association data set was about 20 minutes. The reason for this is that the data set used in the experiment contained about 90,000 rows. The XML schema for this data set generates an XML document for each row, containing all the attributes as XML node elements. Hence, the extractor had to submit 90,000 documents to the data server, thus incurred a large network communication overhead. The data loading time for the coordinate data set was much less because there were about 18,000 rows in total. We observe that the execution time increases linearly as the number of rows increases. This is a performance bottleneck in our current implementation. The network overheads would be more pronounced in a more distributed environment (e.g., the Grid). To reduce the network overhead, multiple documents could be combined into one buffer by the runtime system, and this buffer could be submitted to the server. This requires that the server can handle buffers containing more than one XML document. We are in the process of adding this functionality to the data service component.",
            "cite_spans": [],
            "section": "Performance: Creation of Local Caches and Data Querying ::: Status Report",
            "ref_spans": [
                {
                    "start": 14,
                    "end": 15,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "The number of data rows for the mpd146 data set was 439,942. If a single XML document were created for each row of this data set, the execution time would be around 105 minutes. Since our current implementation does not support concatenation of multiple documents into one buffer and processing of such buffers, we developed an XML schema that allowed us to combine multiple rows of the data set into one XML document. In our experiments, we stored 10,000 rows of the data set in one XML document, thus submitting only 44 documents to the data server. As seen in the figure, the execution time of the data extraction and loading step for mpd146 is less than that of the Gene Association data set because of this optimization.",
            "cite_spans": [],
            "section": "Performance: Creation of Local Caches and Data Querying ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "In the next set of experiments, we look at the query execution performance. The current client interface supports two types of queries. The first query type specifies an SNP identification (rsID) and two strains. In the second type of query, the client inputs a chromosome and a genomic location range. Our experimental results show that the first type of query takes on average about eight times longer to execute than the second type of query. We tried three different queries for each query type; the first query returned one result row, the second 33 result rows, and the last one 241 result rows. The average execution time of the first query type was 76 seconds, whereas the second type of query took 9 seconds on average. When the first type of query is submitted, the mpd146 data set is first searched to find the chromosome corresponding to the rsID and a corresponding genomic location. Next, a region of genetic similarity between the two query strains is computed as follows: multiple accesses to the mpd146 data set are made to find the third mismatch in alleles between the strains in regions downstream and upstream from the location identified by the rsID. On the other hand, the second type of query requires only two accesses to the mpd146 data set to extract the corresponding SNP accession numbers and other information required. In either query, once the range values are determined, the upper and lower bound values are used to find the list of marker accession IDs (see \u25b6, the left-most column). The marker accession IDs are used as join attributes to extract information from other databases, where the marker accession ID of a data element matches one of the marker accession IDs in the list obtained from the coordinate data set. \u25b6 shows the execution time of the second type of query as the query size is scaled up. We executed three different queries in this experiment; the number of rows returned was 27, 161, and 472 rows. The timing numbers are the average execution time over four runs of each query size. As seen from the figure, the execution time increases in proportion to the size of the query results. The standard deviations were 0.15 (27 rows), 0.04 (161 rows), and 0.73 (472 rows) seconds in these experiments.  ",
            "cite_spans": [],
            "section": "Performance: Creation of Local Caches and Data Querying ::: Status Report",
            "ref_spans": [
                {
                    "start": 1493,
                    "end": 1494,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1756,
                    "end": 1757,
                    "mention": "\u25b6",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "There are several issues that have not been addressed in our current implementation. The first issue stems from the use of XPath as our querying language. Although our system can support XPath queries of any complexity, the XPath language does not support joins on XML databases; it provides the basic data-subsetting capabilities. In our implementation, queries involving joins between different data sources have been implemented as client side operations; that is, individual data sets are accessed using XPath queries and any joins between the data subsets are performed in the client application. We are currently working on integrating a subset of XQuery functionality (i.e., support for XQuery FLWR expressions) in our framework. This will make it possible to support more complex queries without implementing client side extensions.",
            "cite_spans": [],
            "section": "Limitations ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "Another limitation of our current implementation is that it does not support integration at the semantic level. It provides the core support needed to enable management of data types and data instances. A semantic integration component can be layered on this core support. However, we recognize that establishing semantically meaningful relationships between data elements from different resources is a complex issue that requires innovative solutions in the semantic layer. An approach to address semantic interoperability is to standardize common data elements, controlled vocabularies, and taxonomies and implement these standards in a framework. There are a number of standardization efforts such as SNOMED (Systematized Nomenclature of Medicine), LOINC (Logical Observation Identifiers Names and Codes), and DICOM (Digital Imaging and Communication in Medicine) for naming and description of data attributes and data. In cancer research, the NCI Center for Bioinformatics has developed the Cancer Data Standards Repository and the Enterprise Vocabulary Services to serve as a controlled vocabulary source in order to enable common semantics across related databases. There is also an ongoing effort in the caBIG initiative to define common data elements and controlled vocabularies to support multi-institutional basic and clinical cancer research projects. We have implemented a prototype service in Mobius to support management of semantic information using resource description framework (RDF). We plan to extend this prototype and integrate it in our software system. We also plan to use the caBIG technologies as they become available to provide support for management and use of controlled vocabularies as well as common data elements.",
            "cite_spans": [],
            "section": "Limitations ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "A performance bottleneck in our current implementation is the data-loading overhead. A network overhead is incurred if data extractors and Mako servers are located on different machines. The data-loading overhead also stems from the fact that during data loading, each row in the source database is encoded in an XML document and submitted to a Mako server. This overhead could be reduced by combining multiple documents into a message buffer that is submitted to the server. Our current implementation does not support multiple documents in a single buffer. We plan to incorporate this performance optimization into our implementation in future. Another approach to reduce the data loading overhead would be to colocate extractors and Mako-servers. We also observed that the cost of executing the first type of query (as explained in the section \u201cPerformance: Creation of Local Caches and Data Querying\u201d is expensive. This is because of the need for multiple accesses to the database to find the bounds for the region of genetic similarity. This overhead could be reduced by implementing data prefetching and in-memory caching. In this strategy, the entire set of attributes required for computing the bounds would be prefetched from the database and cached in an in-memory data structure (e.g., an array) in the application client when the client is started. This would reduce the overhead of querying the database multiple times. However, a caching mechanism would need to be implemented in the client to manage the in-core data structure if the set of required attributes did not fit in memory.",
            "cite_spans": [],
            "section": "Limitations ::: Status Report",
            "ref_spans": []
        },
        {
            "text": "In this paper, we presented the application of an XML-based generic metadata and data management system for management and integration of data in a biomedical application. XML has become a de facto standard for representing structured and semistructured data sets in heterogeneous and distributed environments. The software infrastructure presented in this paper provides core services and common protocols for (1) distributed, but coordinated, managing of metadata definitions, (2) exposure of subsets of data stored in ad hoc data warehouses and enterprise information systems, and (3) on-demand creation of databases along with management of complex data analysis workflows. These capabilities make it possible to support the management and querying of distributed collections of heterogeneous biomedical data sets and integration of such data sources in a unified framework. This type of common distributed data environment, with strongly typed data, can enable implementation of applications that can remove barriers to better synthesis and analysis of information in translational research.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "In this work, the data management and integration framework allowed us to manage and query much larger data sets from phylogenetic tree optimizations, apply CCT on large data sets, and integrate data from external repositories efficiently. Without the ability to create and manage local caches of public databases, it would have required the application to submit large numbers of queries to the respective database servers, thus incurring a lot of network overhead and load on those servers. Moreover, the software system enabled the researchers to share data in a more uniform way and to execute complex queries on large data sets. This was a major step forward from using simple shell and awk scripts to manage, filter, and integrate data generated and referenced in the application.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "The development of the application involved close collaboration between a group of biomedical researchers and a group of information technology developers. In this collaboration, we observed that the analysis of the requirements was the most challenging part. This was mainly because it required a good understanding of the scientific application as well as the capabilities of the software system by both groups. This process proved that the development of good software applications to support biomedical research is only possible through close collaboration of application domain experts and information technology developers.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "Although we focused on one application in this paper, our system is extensible and customizable for other applications. In another collaborative project, we have developed an application to support data sets from SNPlex analyses. In a follow-up to that implementation, we are developing a quality control application that will enable integration of data obtained from multiple experimental and bioinformatics analysis techniques done on the same or overlapping groups of samples. The system will allow a user to query data based on individual or groups of data collection/analysis techniques, perform joins between different data subsets, and carry out statistical analyses on the selected data. For example, a user will be able to query for all samples, for which at least one data collection/analysis technique has generated a different result. Using our framework, we are also implementing an application similar to the application in this paper for study of coronaviruses and influenza A viruses. The application will support management and analysis of data from phylogenetic tree optimizations, results from bioinformatics analysis methods, and integration of data obtained from public repositories to the overall workflow. We plan to report on the status of these applications in a future paper.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "The Mobius framework presented in the paper is available for download from our Web site (www.bmi.osu.edu). We plan to make the application implementation available for download in future after further performance testing and improvement are carried out and a user guide for the application has been written. In the meantime, interested readers are encouraged to contact the authors of this paper.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: List of Data Sets Currently Managed by the Data Management and Integration System\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Figure 1.: (Top) The data model for the output from POY, a program for phylogenetic tree optimization. The rectangles and arrows represent the element types and parent-child relationships. The contents of the square brackets denote the attributes associated with a node type. For example, a branch element type has attributes for the ancestor node, the descendant node, and the minimum and maximum length as computed by the POY program. (Bottom) An annotated example of section of tabular POY output depicting inferred transformations in three characters on a branch of a phylogenetic tree. Character 1 is a phenotype that changed from state 1 in the ancestor (HTU6) to state 2 in the descendant (strain C57BL10J). This change could be from normal to elevated non\u2013high-density lipoprotein cholesterol plasma levels. Character 2 is a single nucleotide polymorphism (SNP), in which change could have occurred along this branch but is ambiguous due to missing data. Character 3 is an SNP, in which a transition mutation occurs. This could be SNP rs3023213 as depicted in \u25b6.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Figure 2.: Two views of the same phylogenetic tree of females of mouse strains displaying correlated changes of a phenotype and a genotype across 15 mouse strains. The right tree depicts phenotypic change in non\u2013high-density lipoprotein (non-HDL) cholesterol plasma levels in female mice after six weeks of atherogenic diet. Black branches indicate strains (C57BL/6J and CAST/EiJ) with non-HDL levels greater than one standard deviation (sd) above the mean after treatment. Genotype observations for each strain for the SNP of interest (rs3023213; T or C) are indicated on the left tree. Boxes at the terminal branches of the trees indicate genotype or phenotype observations in databases for those strains. Concentrated changes test results for this phenotype\u2013genotype correlation differ for females (p = 0.004) and males (p = 0.088) (not shown).",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Figure 3.: Data models for the HMD Human Sequence (left) and mpd146 (right) data sets.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Figure 4.: A view of the Client Interface after a query has been executed for a single nucleotide polymorphism (rs3023213) correlated with higher non\u2013high-density lipoprotein cholesterol levels (see also \u25b6). Within this block of genetic similarity between the query strains (C57BL/6J and CAST/EiJ), a candidate gene, NNMT (nicotinamide N-methyltransferase), for the trait was noted.",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Figure 5.: The execution time of the data set extraction and loading step for the gene association, mpd146, and coordinate data sets.",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Figure 6.: Query execution time as the query size is scaled.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "Annu Rev Genomics Hum Genet.",
            "volume": "4",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": 1996,
            "venue": "ACM Comput Surv.",
            "volume": "28",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": 2002,
            "venue": "Computer.",
            "volume": "35",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": 1997,
            "venue": "Int J High Performance Comput Appl.",
            "volume": "11",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "Parallel Process Lett.",
            "volume": "13",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": 2005,
            "venue": "J Am Med Inform Assoc.",
            "volume": "12",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": 2001,
            "venue": "Science.",
            "volume": "292",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": 2004,
            "venue": "Science.",
            "volume": "306",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": 2004,
            "venue": "PLoS Biol.",
            "volume": "2",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": 2001,
            "venue": "Science.",
            "volume": "294",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": 2000,
            "venue": "Annu Rev Genet.",
            "volume": "34",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": 1993,
            "venue": "J Lipid Res.",
            "volume": "34",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": 1985,
            "venue": "Am Nat.",
            "volume": "125",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": 1990,
            "venue": "Evolution.",
            "volume": "44",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "J Appl Physiol.",
            "volume": "94",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "",
            "authors": [],
            "year": 2005,
            "venue": "Nat Rev Genet.",
            "volume": "6",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "",
            "authors": [],
            "year": 2003,
            "venue": "J Appl Physiol.",
            "volume": "95",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "",
            "authors": [],
            "year": 1996,
            "venue": "Pharmacogenetics.",
            "volume": "6",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "",
            "authors": [],
            "year": 1990,
            "venue": "Clin Chim Acta.",
            "volume": "186",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "",
            "authors": [],
            "year": 2005,
            "venue": "Am J Hum Genet.",
            "volume": "76",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF49": {
            "title": "",
            "authors": [],
            "year": 2002,
            "venue": "J Biol Chem.",
            "volume": "277",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "",
            "authors": [],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "",
            "authors": [],
            "year": 1997,
            "venue": "SIGMOD Rec.",
            "volume": "26",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "",
            "authors": [],
            "year": 1992,
            "venue": "Communications of the ACM.",
            "volume": "35",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF53": {
            "title": "",
            "authors": [],
            "year": 2000,
            "venue": "ACM Comput Surv.",
            "volume": "32",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}